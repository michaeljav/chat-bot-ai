# Start everything:

`docker compose up -d --build`

# Live-Coding Assignment (Interview Session)

## Goal

Build a **tiny chat application** in two partsâ€”backend (NestJS) and frontend (React)â€”that lets a user send a message and receive an automated reply.

---

--LLM HAGA BUSQUEDA EN INTERNET. PUNTO EXTRA.

SEBASTIAN
COPIA A NAYELI

## Part A â€“ Backend (NestJS)

| Requirement          | Details                                                                                                              |
| -------------------- | -------------------------------------------------------------------------------------------------------------------- |
| **Route**            | `POST /chat`                                                                                                         |
| **Request body**     | `{ "message": "string" }` (JSON)                                                                                     |
| **Validation**       | Reject empty or missing `message` with **400 Bad Request**                                                           |
| **Response**         | `{ "reply": "string" }` â€“ for now, any deterministic reply (e.g. echo the message in uppercase or prefix `"Bot: "`). |
| **Tech constraints** | _TypeScript only_ Â· Use NestJS controllers, services, DTOs & `ValidationPipe` Â· No databaseâ€”everything in memory     |
| **Bonus**            | Simple rate-limit guard (â‰¤ 5 req/min per IP) Â· Lightweight `Dockerfile` exposing port `3000`                         |

**What weâ€™ll evaluate**

1. Project structure (`app.module.ts`, `chat.controller.ts`, `chat.service.ts`).
2. Correct HTTP status codes and DTO validation.
3. Code readability & running commentary while you code.
4. (Bonus) Containerization quality.

---

## Part B â€“ Frontend (React)

| Requirement   | Details                                                                                      |
| ------------- | -------------------------------------------------------------------------------------------- |
| **Component** | `ChatBox` (React 18+, hooks, TypeScript)                                                     |
| **UI**        | Text input + **Send** button Â· Scrollable message area (user on right, bot on left is fine). |
| **Behaviour** | 1. `POST /chat` with the user message.                                                       |

2. Show â€œTypingâ€¦â€ while waiting.
3. Append bot reply to conversation.
4. Clear & refocus the input. |
   | **Error handling** | Inline error for 400 (â€œMessage cannot be empty.â€) & network failures (â€œConnection lost, please retry.â€). |
   | **Styling** | Minimal CSS or Tailwindâ€”no external UI kits needed. |
   | **Bonus** | Auto-scroll to newest message Â· Disable **Send** when input is empty Â· Submit with **Enter** key. |

**What weâ€™ll evaluate**

1. Clean state management (`useState`, `useEffect`).
2. Type safety for request/response shapes.
3. Smooth UX & graceful error handling.
4. Code clarity and naming.

â± **Suggested time**: 15 min backend + 15 min frontend.

---

# Take-Home Assignment: LLM Integration

> **Extend your chat app by connecting it to an LLM service (Google Gemini is free) so users can ask domain-specific questions and get meaningful answers.**

## 1 â€“ Choose an LLM Provider

Google Gemini preferred, but OpenAI, Cohere, etc. are fine if you have access.

## 2 â€“ Backend Changes (NestJS)

- Create `LlmService` that forwards the userâ€™s **message** to the LLM and retrieves the answer.
- Use **env vars** (`LLM_API_KEY`, `LLM_MODEL`, â€¦)â€”do **not** hard-code secrets.
- Return the LLMâ€™s reply as `{ "reply": "â€¦" }`.
- Keep existing DTO validation.

## 3 â€“ Domain Focus

Prompt the model to act as an **expert in a single topic** of your choice (e.g. Argentine/Brazilian/Dominican cuisine, front-end testing, classic movies). Off-topic questions should trigger a polite refusal or redirection.

## 4 â€“ Frontend Changes (React)

No UI overhaul neededâ€”just display the richer reply and keep the loading indicator.

## 5 â€“ Testing

- Supply **three sample questions** (with expected answer snippets) in your README.
- Optional: a Jest test mocking the LLM call.

## 6 â€“ Delivery

- Push to a public repo or share a zip.
- Include a concise **README** with setup steps (`npm install`, `docker compose up`, `env.example`).

## 7 â€“ Bonus Points

- Stream tokens to the client for a â€œtypingâ€ effect.
- Rate-limit to protect your free quota.

ğŸ•’ **Deadline:** 48 -70 hours (let us know if you need an extension).  
Good luckâ€”and have fun exploring LLMs!
