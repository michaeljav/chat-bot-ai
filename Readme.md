# Quick Start â€” Chat Project (Backend + Frontend)

This mini tutorial shows how to get an LLM API key, run everything with Docker, or run backend and frontend locally.

---

## Prerequisites

- **Docker** and **Docker Compose**
- **Node.js** (v20.19+ recommended) and **npm** (only if running locally without Docker)

---

## 1) Get your LLM API key (Google Gemini)

If you want to use the LLM features and search the web, you need an API key.

**Quick link:** https://aistudio.google.com/app/apikey

**Steps:**

1. Open **Google AI Studio â†’ API Keys**: https://aistudio.google.com/app/apikey
2. Sign in with your Google account.
3. Click **Create API key** and copy the key.
4. You now have your `LLM_API_KEY`.

> **Tip:** Save it in your environment (e.g., `.env` for the backend or your Docker Compose env).

**Example `.env` for the backend (`chat-api/.env`):**

```ini
# Required for LLM/web-search features
LLM_API_KEY=your_gemini_api_key_here

# Optional: domain/topic focus used by your model (if your app supports it)
# LLM_DOMAIN=Dominican cuisine
```

---

## 2) Start everything with Docker (recommended)

From the project root (where `docker-compose.yml` lives), run:

```bash
docker compose up -d --build
```

Once itâ€™s up:

- **Frontend:** http://localhost:8080
- **Swagger UI:** http://localhost:8080/docs  
  _(or http://localhost:3000/docs if you exposed the API directly)_

---

## 3) Start locally without Docker

### Backend

```bash
cd chat-api
npm i
npm run start:dev
```

- Swagger UI will usually be at `http://localhost:3000/docs` (unless you proxy via the frontend).

### Frontend

In a second terminal:

```bash
cd ../chat-frontend
npm i
npm run dev
```

- Open **http://localhost:5173**

---

## 4) Test the app

### Example prompts for online search

- â€œGive me two news stories about Dominican gastronomy from **today**, and cite the sources with links.â€
- â€œFind me a URL about Dominican food from today.â€

> If your app has a toggle for â€œUse LLMâ€ or â€œSearch the web,â€ turn it on before asking.

---

## Troubleshooting

- If a page does not load, check the container logs:
  ```bash
  docker compose logs -f
  ```
- Make sure your `LLM_API_KEY` is set in the environment used by the backend.
- Port tips:
  - Frontend (Docker): `8080`
  - Frontend (dev): `5173`
  - Backend (typical): `3000`

---

**End of mini tutorial** âœ…

# Live-Coding Assignment (Interview Session)

## Goal

Build a **tiny chat application** in two partsâ€”backend (NestJS) and frontend (React)â€”that lets a user send a message and receive an automated reply.

---

--LLM HAGA BUSQUEDA EN INTERNET. PUNTO EXTRA.

SEBASTIAN
COPIA A NAYELI

## Part A â€“ Backend (NestJS)

| Requirement          | Details                                                                                                              |
| -------------------- | -------------------------------------------------------------------------------------------------------------------- |
| **Route**            | `POST /chat`                                                                                                         |
| **Request body**     | `{ "message": "string" }` (JSON)                                                                                     |
| **Validation**       | Reject empty or missing `message` with **400 Bad Request**                                                           |
| **Response**         | `{ "reply": "string" }` â€“ for now, any deterministic reply (e.g. echo the message in uppercase or prefix `"Bot: "`). |
| **Tech constraints** | _TypeScript only_ Â· Use NestJS controllers, services, DTOs & `ValidationPipe` Â· No databaseâ€”everything in memory     |
| **Bonus**            | Simple rate-limit guard (â‰¤ 5 req/min per IP) Â· Lightweight `Dockerfile` exposing port `3000`                         |

**What weâ€™ll evaluate**

1. Project structure (`app.module.ts`, `chat.controller.ts`, `chat.service.ts`).
2. Correct HTTP status codes and DTO validation.
3. Code readability & running commentary while you code.
4. (Bonus) Containerization quality.

---

## Part B â€“ Frontend (React)

| Requirement   | Details                                                                                      |
| ------------- | -------------------------------------------------------------------------------------------- |
| **Component** | `ChatBox` (React 18+, hooks, TypeScript)                                                     |
| **UI**        | Text input + **Send** button Â· Scrollable message area (user on right, bot on left is fine). |
| **Behaviour** | 1. `POST /chat` with the user message.                                                       |

2. Show â€œTypingâ€¦â€ while waiting.
3. Append bot reply to conversation.
4. Clear & refocus the input. |
   | **Error handling** | Inline error for 400 (â€œMessage cannot be empty.â€) & network failures (â€œConnection lost, please retry.â€). |
   | **Styling** | Minimal CSS or Tailwindâ€”no external UI kits needed. |
   | **Bonus** | Auto-scroll to newest message Â· Disable **Send** when input is empty Â· Submit with **Enter** key. |

**What weâ€™ll evaluate**

1. Clean state management (`useState`, `useEffect`).
2. Type safety for request/response shapes.
3. Smooth UX & graceful error handling.
4. Code clarity and naming.

â± **Suggested time**: 15 min backend + 15 min frontend.

---

# Take-Home Assignment: LLM Integration

> **Extend your chat app by connecting it to an LLM service (Google Gemini is free) so users can ask domain-specific questions and get meaningful answers.**

## 1 â€“ Choose an LLM Provider

Google Gemini preferred, but OpenAI, Cohere, etc. are fine if you have access.

## 2 â€“ Backend Changes (NestJS)

- Create `LlmService` that forwards the userâ€™s **message** to the LLM and retrieves the answer.
- Use **env vars** (`LLM_API_KEY`, `LLM_MODEL`, â€¦)â€”do **not** hard-code secrets.
- Return the LLMâ€™s reply as `{ "reply": "â€¦" }`.
- Keep existing DTO validation.

## 3 â€“ Domain Focus

Prompt the model to act as an **expert in a single topic** of your choice (e.g. Argentine/Brazilian/Dominican cuisine, front-end testing, classic movies). Off-topic questions should trigger a polite refusal or redirection.

## 4 â€“ Frontend Changes (React)

No UI overhaul neededâ€”just display the richer reply and keep the loading indicator.

## 5 â€“ Testing

- Supply **three sample questions** (with expected answer snippets) in your README.
- Optional: a Jest test mocking the LLM call.

## 6 â€“ Delivery

- Push to a public repo or share a zip.
- Include a concise **README** with setup steps (`npm install`, `docker compose up`, `env.example`).

## 7 â€“ Bonus Points

- Stream tokens to the client for a â€œtypingâ€ effect.
- Rate-limit to protect your free quota.

ğŸ•’ **Deadline:** 48 -70 hours (let us know if you need an extension).  
Good luckâ€”and have fun exploring LLMs!
